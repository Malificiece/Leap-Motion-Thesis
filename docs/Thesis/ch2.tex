\chapter{Literature Review}

\section{Mid-Air Pointing}
Finding alternatives to word separation for mid-air text-entry heavily involves utilizing mid-air pointing to give character-level precision and determine the gesture-shapes being drawn. In the past, much of the research on mid-air pointing was done through casting rays onto an assortment of distant display screens \cite{ref_large_display_pointing,ref_air_pointing,ref_ray_pointing_large_displays,ref_shadow_reaching,ref_freehand_pointing_large_displays,ref_large_screen_pointing_gestures}, however the methods vary vastly in their efficiency, precision, accuracy, and fatigue. Even something as simple as hand jitter was a limiting factor to these sub-par detection techniques for mid-air pointing. These issues have recently began to be overcome thanks to products like the Leap Motion Controller which can deliver tracking precision on the sub-millimeter level and implements techniques to detect and stabilize for hand jitter \cite{ref_leap_device_evaluation_1,ref_leap_device_evaluation_2} Another major factor that had heavily affected these ray-casting techniques was their dependence on the distance from the display screen \cite{ref_mid_air_text_large_displays}. Markussen et al. chose to minimize the distance dependence by projecting movement orthogonally onto the display to maintain a constant control-display ration at the cost of the user's reach \cite{ref_selection_based_mid_air}. A second option, the option used in this study, was to instead project an interaction plane in front of the user within their reach that was independent from the display space \cite{ref_alvin_thesis,ref_darren_thesis,ref_leap_pointing_device}. The major issue in decoupling the motor space and display space was that participants had to recouple the two spaces mentally, addressing the principle of stimulus-response compatibility \cite{ref_vulture,ref_stimulus_response_compatibility}. Although it was difficult to recouple these spaces, promising results have shown that the effects can be minimized using different techniques to calibrate the motor space and provide for efficient pointing throughput \cite{ref_alvin_thesis,ref_darren_thesis}.

\section{Barehanded Interaction}
% TODO: SWAP THIS SECTION AROUND SO THAT WE HAVE A REASON FOR SAYING WHY WE SHOULD
% USE BAREHANDED INTERCATION BECAUSE WHO CARES THAT ITS BETTER
Unlike Vulture, which utilized a glove with reflective markers, this study decided to make interactions as natural as possible and use a barehanded approach for gestural interactions. Users should not be forced to wear a glove or reflective markers for interactions \cite{ref_natural_interactions}; this was known as the design principle: ``Come As You Are'' \cite{ref_come_as_you_are}. The barehanded approach for gestural interactions have been shown to be better for gestures than traditional input devices \cite{ref_barehanded_interaction} and have been used for natural and relaxed gestures for computer interaction \cite{ref_natural_relaxed_gestures}. More recently, the Leap Motion Controller has been shown to be an acceptable tracking device for barehanded gestural interactions \cite{ref_alvin_thesis,ref_darren_thesis,ref_leap_device_evaluation_1,ref_leap_device_evaluation_2,ref_leap_pointing_device}.

\section{Mid-Air Text-Entry}
% TODO: CLAIRFY THE ``EFFECTIVENESS'' STATEMENT
Since its inception, many techniques have been used for mid-air text-entry that all vary in effectiveness, namely error rate and text-entry rate. Mid-air text-entry has evolved along-side both the techniques for normal text-entry and the hardware available for tracking tools and hands.

\subsection{Selection-Based}
% TODO: NEED TO PUT REFERENCES AFTER EACH STATEMENT
% BE CLEARER ON THE LAST PART TO WHICH ONE IS WHICH
Some of the earliest work on mid-air text entry focused mainly on selection-based techniques; such efforts were centered around TODO: restructure and find reference for each of these, hovering \cite{ref_visual_touchpad}, or various other techniques and layouts for character selection \cite{ref_mid_air_text_large_displays,ref_selection_based_mid_air}. The major limitation to these techniques was that they use single-input text-entry which is slow. One of the earliest implementations, even similar to what was constructed in this experiment was Visual Touchpad \cite{ref_visual_touchpad}; however, it was extremely limited by the hardware and processing of the time, requiring a 3-second period of hovering to select an individual key. More recent efforts have recently performed better for selection-based techniques reaching rates as high as 18.9 WPM \cite{ref_mid_air_text_large_displays} and 13.2 WPM \cite{ref_selection_based_mid_air} by utilizing new selection-based methods for text-entry utilizing a QWERTY-based layout. These studies; however, were limited, the former providing no character production on erroneous input \cite{ref_mid_air_text_large_displays} and the latter using orthogonal projection of the hand's position onto the display \cite{ref_selection_based_mid_air}.

\subsection{Gesture-Based}
More recent efforts, as technology improved and gestures became more easily analyzed and detected, have focused on looking at gesture-based techniques for mid-air text-entry. One of the prominent examples was AirStroke \cite{ref_airstroke} which utilized a Graffiti-based alphabet for text-entry and achieved text-entry rates as high as 11.0 WPM. It was also shown, implemented by using an Xbox Kinect, that gestures could be recognized continuously on an interaction plane that was projected in front of the user \cite{ref_continuous_recognition} and could be used for recognizing gestures for text-entry \cite{ref_graffiti_vs_unistroke}. The idea of utilizing an interactive gesture-space in front of the user has been further supported by recent research using the Leap Motion Controller \cite{ref_alvin_thesis,ref_darren_thesis}. Other alternatives have attempted to use hand-writing as a gesture-based technique in mid-air \cite{ref_air_handwriting,ref_air_writing_continuous_recognition,ref_mid_air_text_entry_handwriting,ref_detecting_handwritten_characters}; however, even surface-based hand-writing was shown to be slow, limited to around 15 WPM \cite{ref_handprinting_alternatives}.

\subsection{Word-Gesture Keyboards}
% TODO: adjust last sentence and find meaning
Markussen et al. developed the first and only mid-air word-gesture keyboard, Vulture \cite{ref_vulture}. The Vulture keyboard was able to achieve the highest text-entry rates seen for mid-air in two studies, reaching 20.6 WPM with repeated sessions and 28.1 WPM with training on short phrases \cite{ref_vulture}. Vulture used one of the few publicly available and better-known implementations of work-gesture keyboards that was developed for phones, the SHARK$^2$ \cite{ref_shape_writing,ref_shark_wgk}, and used a glove with reflective markers for tracking pinching-based gestures on an orthogonally projected display \cite{ref_selection_based_mid_air}.

\section{Word-Gesture Keyboards}
Word-gesture keyboards have seen their emergence along-side the growth of touch-based, hand-held devices, now being shipped as the standard keyboard on many smart phone devices. Originally, word-gesture keyboards were known as shape-writing keyboards because of the process of tracing a word's gesture-shape as opposed to conventional single-input or multi-tap text-entry \cite{ref_shape_writing,ref_the_word_gesture_keyboard,ref_shapewriter_iphone,ref_shark_wgk,ref_shorthand_writing}. Word-gesture keyboards have garnered text-entry rates as high as 25 WPM after 35 minutes of practice for beginners and up to 46.5 WPM for well-practice phrases \cite{ref_shape_writing}.

A traditional part of implementing word-gesture keyboards was to include shape-recognition algorithms in order to best determine what words the user was trying to create \cite{ref_shape_writing,ref_the_word_gesture_keyboard,ref_shapewriter_iphone,ref_shark_wgk,ref_shorthand_writing}. One of the most thorough and available examples of this was SHARK$^2$ \cite{ref_shape_writing,ref_shark_wgk}, as was used in Vulture \cite{ref_vulture}.

Word-recognition has been proven to work for word-gesture keyboards; therefore, it sits outside the goals of this study. Since it was known in advanced what words and characters that a user was going to be attempting to produce, it was believed a word-gesture keyboard could be pseudo-implemented without word-recognition. To ensure that the results were accurate and to analyze the effectiveness of a pseudo-implementation of a word-gesture keyboard, both a pinching-based and touch-based implementation were also examined.

\section{Text-Entry Evaluation}
\subsection{Text-Entry Rates}
The most important measure for testing the various methods of word separation was by monitoring the text-entry rate. Achieving text-entry rates on par with or greater than those produced in Vulture will shed light on improved techniques for mid-air, text-entry interactions. Text-entry rate was measured in Words Per Minute \cite{ref_wpm_word_gesture_formula}; however, including the time to produce the first character must be included when timing word gestures \cite{ref_wpm_word_gesture_timing}.

\subsection{Error Rates}
The Vulture keyboard uses Minimum Word Distance to primarily detect error rates in the phrases that were transcribed \cite{ref_vulture}. However, due to the pseudo-implementation of the word-gesture keyboard and the fact that characters and errors could be produced mid-gesture, some character level error rates were also observed. These error rates were the Keystrokes Per Character, Minimum String Distance, and Total Error Rate \cite{ref_error_rates}. Of the error rates introduced, the Keystrokes Per Character method was not without its flaws, as noted by Soukoreff and MacKenzie; however, because characters were produced during the gesturing process, this might give some insight on the production rate of erroneous characters when using the pseudo word-gesture keyboard implementation.

\subsection{Correctness}
Word-gesture keyboards heavily rely on shape-recognition algorithms in order to determine which words were recognized and produced; however, many of these studies analyze error rates of the final transcribed text, rather than evaluating the correctness between the user-generated gesture and the shape of the intended-word. The Fr\'echet Distance gives us the ability to do just that. The Fr\'echet Distance between two curves $P$ and $Q$, or in this case gesture-shapes, was best defined as the minimum length leash needed to walk a dog when the person walks along $P$ and the dog walks along $Q$ \cite{ref_frechet}. The lower the calculated distance was between the two paths, the closer the two paths produced were to each other, providing a way to rate the correctness of the generated paths.

\section{Summary}
% TODO: Need summary of what was discussed in lit review